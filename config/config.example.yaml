# PS Agents Configuration

# Server Configuration
server:
  host: "0.0.0.0"
  port: 8080
  grpc_port: 50051

# Pipeline Configuration
pipeline:
  batch_size: 100
  max_workers: 4
  timeout: "30s"

devmode:
  enabled: true
  max_messages: 1000

# Graph Database Configuration
graphdb:
  type: "neo4j"  # or "janusgraph"
  host: "localhost"
  port: 7687
  username: "neo4j"
  password: "password"
  similarity_anchors: 10   # see README.md for more details
  semantic_frontier: 10    # see README.md for more details

# Embeddings Configuration
# Supports local Qdrant vector database storage
qdrant:
  enabled: true
  path: "data/qdrant"  # Local file path for Qdrant storage
  collection_name: "embeddings"
  vector_size: 1024
  distance: "Cosine"  # Options: Cosine, Euclid, Dot
  on_disk_payload: true  # Store payload on disk to reduce memory usage
  optimize_for_disk_access: true  # Optimize for disk access patterns

embeddings:
  model: "mxbai-embed-large"
  dimension: 1024
  cache_size: 1000
  similarity_threshold: 0.8
  endpoint: "http://localhost:11434/api/embeddings"  # Separate embedding endpoint

# LLM Configuration
llm:
  # Common LLM settings
  provider: "openai"
  timeout_seconds: 600
  max_tokens: 4096
  temperature: 0.7
  inference_batch_size: 4
  llm_threshold:
    min: 0.3
    max: 0.8
  system_prompt_file: "data/prompts/system.json"  # Load system prompt from external file
  inference_system_prompt_file: "data/prompts/inference_system.json"  # Load inference system prompt from external file
  # Provider-specific configurations
  providers:
    ollama:
      enabled: false
      endpoint: "http://localhost:11434/api/chat"  # Ollama chat endpoint
      model: "llama2"  # Default model that's commonly available
      api_key: "${OLLAMA_API_KEY}"  # If needed

    openai:
      enabled: true
      endpoint: "https://openrouter.ai/api/v1/chat/completions"  # OpenRouter endpoint
      model: "gpt-4o-mini"  # OpenRouter model
      api_key: "${OPENAI_API_KEY}"
      # Override common settings if needed
      # timeout_seconds: 300
      # max_tokens: 2000
      # temperature: 0.5

# Data Configuration
data:
  input_dir: "data/input"
  output_dir: "data/output"
  temp_dir: "data/temp"

# Logging Configuration
logging:
  level: "info"
  format: "json"
  output: "logs/app.log"

# add stages for the ingestion pipeline, such that allows selectively enables/disables stages
ingestion:
  stages:
    - embedding: false
    - semantic_search: false
    - graph_construction: true
    - graph_construction_pass_1: false
    - graph_construction_pass_2: true
    - graph_compression: true

vector:
  provider: ollama
  model: llama2
  batch_size: 100
  chunk_size: 1000
  chunk_overlap: 200

inference:
  max_hops: 3  # Maximum number of hops to traverse in the graph
  max_similarity_anchors: 10  # Maximum number of similarity anchors to use
  min_confidence: 0.7  # Minimum confidence score for relationships
  max_related_messages: 20  # Maximum number of related messages to include
  difficulty_levels:  # Mapping of difficulty levels to confidence thresholds
    easy: 0.8
    medium: 0.6
    hard: 0.4
